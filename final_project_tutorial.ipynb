{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44afbef3",
   "metadata": {},
   "source": [
    "# Final Project Starting Guide\n",
    "\n",
    "Hello everyone, welcome to the final project! This notebook is provided to you to reiterate the rules and guidelines, and give you some starting points.\n",
    "\n",
    "### What we provide\n",
    "\n",
    "In this project, we will provide you with \n",
    "- This starting guide\n",
    "- A working API that you can access under ASU network (i.e., on campus or with VPN)\n",
    "- A starting development data that you can use to develop your agent. It contains 1,000 instances with {domain, input, expected_output}\n",
    "\n",
    "### Your goal\n",
    "\n",
    "In this project, you will implement an inference-time agent to solve reasoning requests, as those provided in the development data. The grading of this project will be effort-based and you will get full credit if you produce the minimum deliverables below, with subject to the rules and requirements below.\n",
    "\n",
    "#### Minimum Deliverables\n",
    "\n",
    "1. A working agent loop (in the form of a Github project) that the TA can run, and implements *at least three* inference-time algorithms or techniques.\n",
    "2. Outputs from your agent on the released test data (see important dates). \n",
    "3. A short one-page report on how your agent works, and pointer to important techniques (referece to code blocks).\n",
    "\n",
    "#### Rules and Requirements\n",
    "1. You must only use our provided API call to access LLMs; meaning that you cannot use any other LLMs in any other way within your agent loop. Some exceptions may be made if you call certain external tools (e.g., Google search) that use some LLMs internally. Please discuss any exceptions with us to avoid penalties up to 50% of the project grade.\n",
    "2. You must not hardcode a full delegation to an external tool (e.g., google_search(input_problem)). Such delegations must be automatically selected/decided by your agent. Hardcode delegations will lead to a zero.\n",
    "3. You cannot use Cursor or any AI coding aids to implement the final project. You can, however, ask LLMs (or other online resources) for conceptual clarification or code examples. Your final project should not contain any blocks of code (i.e., > 3 lines) that are written by AI. Violations will lead to a zero.\n",
    "4. Your agent should be able to run efficiently, with <20 LLM calls per question. Exceptions may be made when you have a complicated agent but please discuss with us. Up to 10% of the project grade may be deducted if we observe very inefficient LLM usages that do not clearly benefit the performance.\n",
    "5. Your agent must run without any requests to any paid services (paid is defined by if the TA has to pay to run it, regardless of whether you actuallly pay for it or not.) Violations will lead to a zero.\n",
    "6. You must submit a Github project link as your code submission. All changes must be tracked and any commits should be within 100 lines of +/- with good messages. Points will be deducted to up to 25% of the project grade if we observe \"magic commits\" or too few commits. \n",
    "\n",
    "\n",
    "### Suggestions\n",
    "1. Start early, please.\n",
    "2. You should consider how you can evaluate whether your output is good enough compared to the provided expected_outputs, and we will not release how we will actually evaluate your outputs; meaning that you have to try to predict how we will evaluate things.\n",
    "3. Start with a basic implementation, and iterate based on mistakes/feedbacks.\n",
    "4. Find more development data, or create your own cases to stree-test your agent. \n",
    "5. You are free to modify any provided code in this starting guide, or not using any of these code at all.\n",
    "\n",
    "### Important dates\n",
    "- **Release of final test data**: 11/25/2025\n",
    "- **Deadline for submitting all deliverables**: 12/05/2025\n",
    "\n",
    "### Extra Credit. \n",
    "The top 20 projects (ranked by performance metrics on the test data and at the TA's discretion of implementation quality) will be given extra credits. The actual credits will be between 1% to 7.5% depending on the ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af7858ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")\n",
    "MODEL = os.getenv(\"MODEL_NAME\", \"bens_model\")\n",
    "\n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer—no explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                max_tokens: int = 128,\n",
    "                                temperature: float = 0.0,\n",
    "                                timeout: int = 60) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers,\n",
    "                             json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            text = data.get(\"choices\", [{}])[0].get(\n",
    "                \"message\", {}).get(\"content\", \"\")\n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6362f9",
   "metadata": {},
   "source": [
    "## 1) Smoke test: direct inference\n",
    "\n",
    "We’ll do a single request with a strict instruction to answer briefly.  \n",
    "*If you see an auth error, set `OPENAI_API_KEY` and (if needed) `API_BASE`/`MODEL_NAME`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d8c02ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: True HTTP: 200\n",
      "MODEL SAYS: 45\n"
     ]
    }
   ],
   "source": [
    "# %% Direct call example\n",
    "demo_prompt = \"What is 17 + 28? Answer with just the number.\"\n",
    "result = call_model_chat_completions(demo_prompt)\n",
    "print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "# Optional: Inspect rate-limit headers if your provider exposes them\n",
    "for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "    if k in result[\"headers\"]:\n",
    "        print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1c568",
   "metadata": {},
   "source": [
    "## 2) A tiny test set (3 questions)\n",
    "\n",
    "We’ll cover:\n",
    "1. **Math reasoning** — inequality solving,\n",
    "2. **Common sense** — buoyancy/ice & water,\n",
    "3. **Logic** — a classic race-position puzzle.\n",
    "\n",
    "We also tightly constrain the required answer forms to enable simple auto‑grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0334e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9840398",
   "metadata": {},
   "source": [
    "## 3) Minimal evaluator\n",
    "\n",
    "We provide some example code to decide whether the agent outputs match the expected outputs, just to give you an idea of how evaluations can be done. You are free to use this code, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ffddeb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2/3 correct\n",
      "❌ math_inequality: expected='8', got='4' (HTTP 200)\n",
      "✅ commonsense_ice: expected='stay the same', got='stay the same' (HTTP 200)\n",
      "✅ logic_race: expected='second', got='second' (HTTP 200)\n"
     ]
    }
   ],
   "source": [
    "# %% Simple normalization and evaluation helpers\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    # Remove surrounding punctuation and extra whitespace\n",
    "    s = re.sub(r\"[^\\w\\s\\-']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # Map common synonyms used in these tests\n",
    "    synonyms = {\n",
    "        \"unchanged\": \"stay the same\",\n",
    "        \"no change\": \"stay the same\",\n",
    "        \"same\": \"stay the same\",\n",
    "        \"second place\": \"second\",\n",
    "        \"2nd\": \"second\",\n",
    "        \"first place\": \"first\",\n",
    "        \"third place\": \"third\",\n",
    "    }\n",
    "    return synonyms.get(s, s)\n",
    "\n",
    "def extract_number(s: str):\n",
    "    # Returns first number occurrence as string if found, else None\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(r\"[-+]?\\d+(\\.\\d+)?\", s)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def grade(expected: str, got: str, kind: str) -> bool:\n",
    "    if kind == \"numeric\":\n",
    "        exp_num = extract_number(expected)\n",
    "        got_num = extract_number(got)\n",
    "        return (exp_num is not None) and (got_num == exp_num)\n",
    "    else:\n",
    "        return normalize_text(got) == normalize_text(expected)\n",
    "\n",
    "def evaluate_tests(tests, model=MODEL):\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            t[\"prompt\"],\n",
    "            system=\"You are a careful solver. Reply ONLY with the final answer, nothing else.\",\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        got = (r[\"text\"] or \"\").strip()\n",
    "        is_correct = grade(t[\"expected\"], got, t[\"type\"])\n",
    "        rows.append({\n",
    "            \"id\": t[\"id\"],\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": got,\n",
    "            \"correct\": is_correct,\n",
    "            \"status\": r[\"status\"],\n",
    "            \"error\": r[\"error\"],\n",
    "        })\n",
    "        # Tiny pacing to be polite to the API\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # Print a small report\n",
    "    correct = sum(1 for x in rows if x[\"correct\"])\n",
    "    print(f\"Score: {correct}/{len(rows)} correct\")\n",
    "    for x in rows:\n",
    "        mark = \"✅\" if x[\"correct\"] else \"❌\"\n",
    "        print(f\"{mark} {x['id']}: expected={x['expected']!r}, got={x['got']!r} (HTTP {x['status']})\")\n",
    "        if x[\"error\"]:\n",
    "            print(\"   error:\", x[\"error\"])\n",
    "    return rows\n",
    "\n",
    "results = evaluate_tests(tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6559aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # Fallback: simple normalization-based equality\n",
    "    norm = lambda s: re.sub(r\"\\s+\", \" \", (s or \"\").strip().lower())\n",
    "    return norm(prediction) == norm(expected_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7bfad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ math_inequality: expected='8', got='4' (HTTP 200)\n",
      "✅ commonsense_ice: expected='stay the same', got='stay the same' (HTTP 200)\n",
      "✅ logic_race: expected='second', got='second' (HTTP 200)\n"
     ]
    }
   ],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    rows = []\n",
    "\n",
    "    for t in tests:\n",
    "        # 1) Get model prediction\n",
    "        r = call_model_chat_completions(\n",
    "            t[\"prompt\"],\n",
    "            system=\"You are a careful solver. Reply ONLY with the final answer, nothing else.\",\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "\n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"id\": t.get(\"id\", \"<unnamed>\"),\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": got,\n",
    "            \"correct\": bool(is_correct),\n",
    "            \"status\": r.get(\"status\"),\n",
    "            \"error\": r.get(\"error\"),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "        if verbose:\n",
    "            mark = \"✅\" if is_correct else \"❌\"\n",
    "            print(f\"{mark} {row['id']}: expected={row['expected']!r}, got={row['got']!r} (HTTP {row['status']})\")\n",
    "            if row[\"error\"]:\n",
    "                print(\"   error:\", row[\"error\"])\n",
    "\n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# Example:\n",
    "results_llm_judge = self_evaluate_tests(tests, verbose=True, model=MODEL, grader_model=MODEL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "948d8800",
   "metadata": {},
   "source": [
    "## My Agent Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55ee70f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dev examples: 1000\n",
      "First dev item:\n",
      "{'input': 'Let $ABCD$ be a convex quadrilateral with $AB = CD = 10$ , $BC = 14$ , and $AD = 2\\\\sqrt{65}$ . Assume that the diagonals of $ABCD$ intersect at point $P$ , and that the sum of the areas of triangles $APB$ and $CPD$ equals the sum of the areas of triangles $BPC$ and $APD$ . Find the area of quadrilateral $ABCD$ .', 'output': '112', 'domain': 'math'}\n"
     ]
    }
   ],
   "source": [
    "# load development data from json\n",
    "import json\n",
    "\n",
    "with open(\"cse476_final_project_dev_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "print(\"Number of dev examples:\", len(dev_data))\n",
    "print(\"First dev item:\")\n",
    "print(dev_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2fcacf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test questions: 6208\n",
      "Template rows: 6208\n"
     ]
    }
   ],
   "source": [
    "# load test data + answer template\n",
    "with open(\"cse_476_final_project_test_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "with open(\"cse_476_final_project_answers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_answers = json.load(f)\n",
    "\n",
    "print(\"Test questions:\", len(test_data))\n",
    "print(\"Template rows:\", len(test_answers))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34ad6d39",
   "metadata": {},
   "source": [
    "Inference Techniques #1 chain of thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "637ab0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought(\n",
    "    task_text: str,\n",
    "    system_message: str = (\n",
    "        \"You are a careful but VERY concise reasoner.\\n\"\n",
    "        \"Dont explain what youre doing do the steps that you need to do\\n\"\n",
    "        \"Solve the problem in at most 5 short steps.\\n\"\n",
    "        \"Do NOT restate the question.\\n\"\n",
    "        \"Use as few words as possible.\\n\"\n",
    "        \"On the last line, write exactly: FINAL ANSWER: <answer>\"\n",
    "    ),\n",
    "    temp: float = 0.4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single chain-of-thought call.\n",
    "    Returns (raw_text, raw_response_dict).\n",
    "    The prompt is optimized to keep reasoning short and avoid token overflow.\n",
    "    \"\"\"\n",
    "\n",
    "    res = call_model_chat_completions(\n",
    "        prompt=task_text,\n",
    "        system=system_message,\n",
    "        model=MODEL,\n",
    "        temperature=temp,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "\n",
    "    raw_text = (res.get(\"text\") or \"\").strip()\n",
    "    return raw_text, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "261f3a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 + 29 = 42  \n",
      "FINAL ANSWER: 42\n"
     ]
    }
   ],
   "source": [
    "txt, _ = chain_of_thought(\"what is 13+29? Just give the number\")\n",
    "print((txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e19475a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(model_output) -> str:\n",
    "    \"\"\"Extract a clean final answer string from the model's output.\"\"\"\n",
    "    import re\n",
    "\n",
    "    # Normalize to string\n",
    "    if model_output is None:\n",
    "        return \"\"\n",
    "    if not isinstance(model_output, str):\n",
    "        model_output = str(model_output)\n",
    "\n",
    "    text = model_output.strip()\n",
    "    lower_output = text.lower()\n",
    "\n",
    "    # Look for \"final answer:\" (case-insensitive, use last occurrence)\n",
    "    if \"final answer:\" in lower_output:\n",
    "        idx = lower_output.rfind(\"final answer:\")\n",
    "        # slice from the original-cased text\n",
    "        final_answer = text[idx + len(\"final answer:\"):].strip()\n",
    "    else:\n",
    "        # Fallback: use the whole output\n",
    "        final_answer = text\n",
    "\n",
    "    # Remove \\boxed{...} if present\n",
    "    if \"\\\\boxed{\" in final_answer:\n",
    "        start = final_answer.find(\"\\\\boxed{\") + len(\"\\\\boxed{\")\n",
    "        end = final_answer.rfind(\"}\")\n",
    "        if end > start:\n",
    "            final_answer = final_answer[start:end].strip()\n",
    "\n",
    "    # Strip surrounding $...$ or \\( \\) LaTeX math wrappers\n",
    "    final_answer = final_answer.strip()\n",
    "    if final_answer.startswith(\"$$\") and final_answer.endswith(\"$$\"):\n",
    "        final_answer = final_answer[2:-2].strip()\n",
    "    elif final_answer.startswith(\"$\") and final_answer.endswith(\"$\"):\n",
    "        final_answer = final_answer[1:-1].strip()\n",
    "    elif final_answer.startswith(r\"\\(\") and final_answer.endswith(r\"\\)\"):\n",
    "        final_answer = final_answer[2:-2].strip()\n",
    "\n",
    "    # Normalize True/False style answers\n",
    "    lower_answer = final_answer.lower()\n",
    "    words = lower_answer.split()\n",
    "    first_word = words[0] if words else \"\"\n",
    "\n",
    "    if \"false\" in lower_answer and \"true\" not in lower_answer:\n",
    "        final_answer = \"False\"\n",
    "    elif \"true\" in lower_answer and \"false\" not in lower_answer:\n",
    "        final_answer = \"True\"\n",
    "    elif first_word in [\"no\", \"no,\", \"no.\"]:\n",
    "        final_answer = \"False\"\n",
    "    elif first_word in [\"yes\", \"yes,\", \"yes.\"]:\n",
    "        final_answer = \"True\"\n",
    "\n",
    "    return final_answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56c5d1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(extract_final_answer(\"some steps\\nFINAL ANSWER: 42\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb7e82c1",
   "metadata": {},
   "source": [
    "Inference Techniques #2 : Self consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8c4c71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# self consistency\n",
    "# reduce number steps from 5 to best 2\n",
    "\n",
    "\n",
    "def self_con(question_text: str,\n",
    "             sample_count: int = 2,\n",
    "             temp: float = 0.4,\n",
    "             ):\n",
    "    \"\"\"self-consistency: run CoT multiple times and pick the most common final answer.\n",
    "    Return (best_answer, all_answers, all_raw_texts). \"\"\"\n",
    "\n",
    "    answers = []\n",
    "    raw_texts = []\n",
    "\n",
    "    for _ in range(sample_count):\n",
    "        raw, _ = chain_of_thought(question_text, temp=temp)\n",
    "        final = extract_final_answer(raw)\n",
    "        answers.append(final)\n",
    "        raw_texts.append(raw)\n",
    "\n",
    "    non_empty = [a for a in answers if a]\n",
    "    if non_empty:\n",
    "        counts = Counter(non_empty)\n",
    "        best = counts.most_common(1)[0][0]\n",
    "    else:\n",
    "        best = \"\"\n",
    "\n",
    "    return best, answers, raw_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "00f48762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 42\n",
      "All: ['42', '42', '42']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "best, all_ans, all_raw = self_con(\"what is 13+29? just give number\", sample_count=3)\n",
    "print(\"Best:\" , best)\n",
    "print(\"All:\", all_ans)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9136e74d",
   "metadata": {},
   "source": [
    "Inference technique #3: Reflection/self_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3415be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference technique #3: Reflection/self_correction\n",
    "def refl(question_text: str,\n",
    "         candidate_answer: str,\n",
    "         temp: float = 0.0):\n",
    "    \"\"\" Reflection: you are a strick grader. \n",
    "    If the answer is deamed wrong, solve again and return the fixed answer.\"\"\"\n",
    "\n",
    "    # 1) T or F\n",
    "    judge_system = (\n",
    "        \"you are a strick grader. \"\n",
    "        \"reply with True or False, no explanation\"\n",
    "    )\n",
    "\n",
    "    judge_prompt = f\"\"\" You are grading the question answer pair.\n",
    "    Return what is True if CANDIDATE_ANSWER is acceptable else return False.\n",
    "\n",
    "    QUESTION:\n",
    "    {question_text}\n",
    "\n",
    "    PREVIOUS_ANSWER:\n",
    "    {candidate_answer}\n",
    "\n",
    "    Answer only True or False. \"\"\"\n",
    "\n",
    "    judge_res = call_model_chat_completions(\n",
    "        prompt=judge_prompt,\n",
    "        system=judge_system,\n",
    "        model=MODEL,\n",
    "        max_tokens=128,\n",
    "        temperature=temp,\n",
    "    )\n",
    "\n",
    "    judge_reply = (judge_res.get(\"text\") or \"\").strip().lower()\n",
    "\n",
    "    if judge_reply.startswith(\"true\"):\n",
    "        return candidate_answer\n",
    "\n",
    "    # if wrong ask for a corrcted answer\n",
    "    fix_system = (\n",
    "        \"You are a probelm solver who is careful.\"\n",
    "        \"Think step by step and give the final answer on the last line\"\n",
    "        \"starting with 'FINAL ANSWER:'.\"\n",
    "    )\n",
    "\n",
    "    fix_prompt = f\"\"\"The previous answer was judged inccorect.\n",
    "\n",
    "    QUESTION:\n",
    "    {question_text}\n",
    "\n",
    "    PREVIOUS_ANSWER:\n",
    "    {candidate_answer}\n",
    "    \n",
    "    solve the question correctly, think step by step and end with: FINAL ANSWER: <correct_answer>\n",
    "    \"\"\"\n",
    "\n",
    "    fix_raw, _ = chain_of_thought(\n",
    "        fix_prompt, system_message=fix_system, temp=0.5)\n",
    "    fixed = extract_final_answer(fix_raw)\n",
    "    return fixed or candidate_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e898cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_con best: 12\n",
      "after reflection: 12\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "q = \"what is 5 + 7? Just give the number.\"\n",
    "best, all_ans, all_raw = self_con(q, sample_count=3)\n",
    "print(\"self_con best:\", best)\n",
    "final =refl(q, best)\n",
    "print(\"after reflection:\", final)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "709f5d78",
   "metadata": {},
   "source": [
    "Boss function to call the 3 techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "97999fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_chat_answer(question: str, model=MODEL) -> str:\n",
    "    \"\"\"\n",
    "    For easier questions, get a single-shot answer.\n",
    "    The model should include a clear 'Final answer:' marker.\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are a concise problem solver. \"\n",
    "        \"Give the correct answer. Include the phrase 'Final answer:' before your final result.\"\n",
    "    )\n",
    "    prompt = f\"\"\"Answer the following question correctly.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Show brief reasoning if needed, but clearly mark the final result with:\n",
    "Final answer: <answer here>\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.0,\n",
    "        max_tokens=128,  # small limit as requested\n",
    "    )\n",
    "    return (r.get(\"text\") or \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "649e0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_difficulty(question: str, model=MODEL) -> int:\n",
    "    \"\"\"\n",
    "    Ask the model to rate the difficulty of the question from 1 (very easy)\n",
    "    to 10 (extremely hard). Returns an int.\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are a difficulty grader for reasoning questions. \"\n",
    "        \"Reply with exactly one integer from 1 to 10 and nothing else.\"\n",
    "    )\n",
    "    prompt = f\"\"\"Rate the DIFFICULTY of the following question on a scale from 1 to 10.\n",
    "\n",
    "1 = very easy, can be answered directly with little or no step-by-step reasoning.\n",
    "10 = extremely difficult, requires deep multi-step reasoning or advanced problem solving.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Answer with only a single integer from 1 to 10.\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.0,\n",
    "        max_tokens=16,   # small number, as requested\n",
    "    )\n",
    "\n",
    "    txt = (r.get(\"text\") or \"\").strip()\n",
    "    # Try to parse an int in [1,10]\n",
    "    try:\n",
    "        diff = int(\"\".join(ch for ch in txt if ch.isdigit()))\n",
    "        if 1 <= diff <= 10:\n",
    "            return diff\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback if parsing fails\n",
    "    return 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0b5f006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 0\n",
      "INPUT Let $ABCD$ be a convex quadrilateral with $AB = CD = 10$ , $BC = 14$ , and $AD = 2\\sqrt{65}$ . Assume that the diagonals of $ABCD$ intersect at point $P$ , and that the sum of the areas of triangles $APB$ and $CPD$ equals the sum of the areas of triangles $BPC$ and $APD$ . Find the area of quadrilateral $ABCD$ .\n",
      "GOLD 112\n",
      "AGENT We are given a convex quadrilateral $ABCD$ with the following side lengths:\n",
      "\n",
      "- $AB = 10$\n",
      "- $CD = 10$\n",
      "- $BC = 14$\n",
      "- $AD = 2\\sqrt{65}$\n",
      "\n",
      "Also, the diagonals intersect at point $P$, and we are told that:\n",
      "\n",
      "$$\n",
      "\\text{Area}(\\triangle APB) + \\text{Area}(\\triangle CPD) = \\text{Area}(\\triangle BPC) + \\text{Area}(\\triangle APD)\n",
      "$$\n",
      "\n",
      "This condition implies that the **sum of the areas of opposite triangles formed by the diagonals is equal**, which is a known geometric property that occurs **only when the diagonals are perpendicular**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Use the property of perpendicular diagonals\n",
      "\n",
      "If the diagonals $AC$ and $BD$ intersect at $P$ and are **perpendicular**, then the area of the quadrilateral $ABCD$ is given by:\n",
      "\n",
      "$$\n",
      "\\text{Area}(ABCD) = \\frac{1}{2} \\cdot AC \\cdot BD\n",
      "$$\n",
      "\n",
      "So, we need to find the lengths of the diagonals $AC$ and $BD$.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Use the given side lengths and the perpendicular diagonals condition\n",
      "\n",
      "We are given:\n",
      "\n",
      "- $AB = 10$\n",
      "- $BC = 14$\n",
      "- $CD = 10$\n",
      "- $AD = 2\\sqrt{65}$\n",
      "\n",
      "Let’s denote the diagonals as $AC = x$ and $BD = y$. Then the area of the quadrilateral is:\n",
      "\n",
      "$$\n",
      "\\text{Area}(ABCD) = \\frac{1}{2} \\cdot x \\cdot y\n",
      "$$\n",
      "\n",
      "We are also told that:\n",
      "\n",
      "$$\n",
      "\\text{Area}(\\triangle APB) + \\text{Area}(\\triangle CPD) = \\text{Area}(\\triangle BPC) + \\text{Area}(\\triangle APD)\n",
      "$$\n",
      "\n",
      "This is only possible if the diagonals are **perpendicular**, so we can use the formula for the area of a quadrilateral with perpendicular diagonals:\n",
      "\n",
      "$$\n",
      "\\text{Area} = \\frac{1}{2} \\cdot AC \\cdot BD\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Use the formula for the area of a quadrilateral with perpendicular diagonals\n",
      "\n",
      "We can also use the formula for the area of a quadrilateral in terms\n",
      "\n",
      "Example 1\n",
      "INPUT A tennis player computes her win ratio by dividing the number of matches she has won by the total number of matches she has played. At the start of a weekend, her win ratio is exactly $0.500$ . During the weekend, she plays four matches, winning three and losing one. At the end of the weekend, her win ratio is greater than $.503$ . What's the largest number of matches she could've won before the weekend began?\n",
      "GOLD 164\n",
      "AGENT 164\n",
      "\n",
      "Example 2\n",
      "INPUT What is the product of the real roots of the equation $x^2 + 18x + 30 = 2 \\sqrt{x^2 + 18x + 45}$ ?\n",
      "GOLD 20\n",
      "AGENT 20\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "for i in range(3):\n",
    "    ex=dev_data[i]\n",
    "    print(\"\\nExample\", i)\n",
    "    print(\"INPUT\", ex[\"input\"])\n",
    "    print(\"GOLD\", ex[\"output\"])\n",
    "    print(\"AGENT\", agent_solve(ex, sc_samples=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dd4aece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(model_output) -> str:\n",
    "    \"\"\"Extract a clean final answer string from the model's output.\"\"\"\n",
    "    import re\n",
    "\n",
    "    # Normalize to string\n",
    "    if model_output is None:\n",
    "        return \"\"\n",
    "    if not isinstance(model_output, str):\n",
    "        model_output = str(model_output)\n",
    "\n",
    "    text = model_output.strip()\n",
    "    lower_output = text.lower()\n",
    "\n",
    "    # Look for \"final answer:\" (case-insensitive, use last occurrence)\n",
    "    if \"final answer:\" in lower_output:\n",
    "        idx = lower_output.rfind(\"final answer:\")\n",
    "        # slice from the original-cased text\n",
    "        final_answer = text[idx + len(\"final answer:\"):].strip()\n",
    "    else:\n",
    "        # Fallback: use the whole output\n",
    "        final_answer = text\n",
    "\n",
    "    # Remove \\boxed{...} if present\n",
    "    if \"\\\\boxed{\" in final_answer:\n",
    "        start = final_answer.find(\"\\\\boxed{\") + len(\"\\\\boxed{\")\n",
    "        end = final_answer.rfind(\"}\")\n",
    "        if end > start:\n",
    "            final_answer = final_answer[start:end].strip()\n",
    "\n",
    "    # Strip surrounding $...$ or \\( \\) LaTeX math wrappers\n",
    "    final_answer = final_answer.strip()\n",
    "    if final_answer.startswith(\"$$\") and final_answer.endswith(\"$$\"):\n",
    "        final_answer = final_answer[2:-2].strip()\n",
    "    elif final_answer.startswith(\"$\") and final_answer.endswith(\"$\"):\n",
    "        final_answer = final_answer[1:-1].strip()\n",
    "    elif final_answer.startswith(r\"\\(\") and final_answer.endswith(r\"\\)\"):\n",
    "        final_answer = final_answer[2:-2].strip()\n",
    "\n",
    "    # Normalize True/False style answers\n",
    "    lower_answer = final_answer.lower()\n",
    "    words = lower_answer.split()\n",
    "    first_word = words[0] if words else \"\"\n",
    "\n",
    "    if \"false\" in lower_answer and \"true\" not in lower_answer:\n",
    "        final_answer = \"False\"\n",
    "    elif \"true\" in lower_answer and \"false\" not in lower_answer:\n",
    "        final_answer = \"True\"\n",
    "    elif first_word in [\"no\", \"no,\", \"no.\"]:\n",
    "        final_answer = \"False\"\n",
    "    elif first_word in [\"yes\", \"yes,\", \"yes.\"]:\n",
    "        final_answer = \"True\"\n",
    "\n",
    "    return final_answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2f080f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_solve(example: dict, sc_samples: int = 2, use_reflection: bool = True) -> str:\n",
    "    \"\"\"Run full agent on example.\n",
    "\n",
    "    1) First, estimate difficulty with a short chat call.\n",
    "    2) If difficulty >= 5: use self_con (CoT + self-consistency) + refl (self-correction).\n",
    "    3) Otherwise: use a single direct chat response.\n",
    "    Returns final answer as plain string.\n",
    "    \"\"\"\n",
    "    question = example.get(\"input\", \"\")\n",
    "\n",
    "    # 1) Estimate difficulty (1–10)\n",
    "    difficulty = estimate_difficulty(question, model=MODEL)\n",
    "    # print(f\"[DEBUG] Estimated difficulty: {difficulty}\")  # optional\n",
    "\n",
    "    # 2a) Hard question → CoT + self-consistency + reflection\n",
    "    if difficulty >= 5:\n",
    "        best, all_ans, all_raw = self_con(\n",
    "            question_text=question,\n",
    "            sample_count=sc_samples,\n",
    "        )\n",
    "\n",
    "        if use_reflection:\n",
    "            final = refl(\n",
    "                question_text=question,\n",
    "                candidate_answer=best,\n",
    "            )\n",
    "        else:\n",
    "            final = best\n",
    "\n",
    "    # 2b) Easy question → single direct chat response\n",
    "    else:\n",
    "        final = direct_chat_answer(question, model=MODEL)\n",
    "\n",
    "    # 3) Return clean final answer string\n",
    "    return extract_final_answer(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cb7f2d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questoins: 6208\n",
      "Jobs (start_idx, end_idx, chunk_id):\n",
      "  (0, 776, 0)\n",
      "  (776, 1552, 1)\n",
      "  (1552, 2328, 2)\n",
      "  (2328, 3104, 3)\n",
      "  (3104, 3880, 4)\n",
      "  (3880, 4656, 5)\n",
      "  (4656, 5432, 6)\n",
      "  (5432, 6208, 7)\n",
      "running max workers = 4\n",
      "[chunk 0] running indices 0..775(total 776)\n",
      "[chunk 1] running indices 776..1551(total 776)\n",
      "[chunk 2] running indices 1552..2327(total 776)\n",
      "[chunk 3] running indices 2328..3103(total 776)\n",
      "[chunk 1] done global index776\n",
      "[chunk 0] done global index0\n",
      "[chunk 2] done global index1552\n",
      "[chunk 3] done global index2328\n",
      "[chunk 0] done global index50\n",
      "[chunk 2] done global index1602\n",
      "[chunk 1] done global index826\n",
      "[chunk 2] done global index1652\n",
      "[chunk 1] done global index876\n",
      "[chunk 0] done global index100\n",
      "[chunk 2] done global index1702\n",
      "[chunk 1] done global index926\n",
      "[chunk 0] done global index150\n",
      "[chunk 2] done global index1752\n",
      "[chunk 0] done global index200\n",
      "[chunk 1] done global index976\n",
      "[chunk 2] done global index1802\n",
      "[chunk 0] done global index250\n",
      "[chunk 1] done global index1026\n",
      "[chunk 3] done global index2378\n",
      "[chunk 2] done global index1852\n",
      "[chunk 0] done global index300\n",
      "[chunk 1] done global index1076\n",
      "[chunk 2] done global index1902\n",
      "[chunk 1] done global index1126\n",
      "[chunk 0] done global index350\n",
      "[chunk 2] done global index1952\n",
      "[chunk 1] done global index1176\n",
      "[chunk 0] done global index400\n",
      "[chunk 1] done global index1226\n",
      "[chunk 2] done global index2002\n",
      "[chunk 0] done global index450\n",
      "[chunk 1] done global index1276\n",
      "[chunk 3] done global index2428\n",
      "[chunk 0] done global index500\n",
      "[chunk 1] done global index1326\n",
      "[chunk 0] done global index550\n",
      "[chunk 1] done global index1376\n",
      "[chunk 0] done global index600\n",
      "[chunk 1] done global index1426\n",
      "[chunk 0] done global index650\n",
      "[chunk 1] done global index1476\n",
      "[chunk 2] done global index2052\n",
      "[chunk 0] done global index700\n",
      "[chunk 1] done global index1526\n",
      "[chunk 3] done global index2478\n",
      "[chunk 1] finished with 776 answers\n",
      "[chunk 4] running indices 3104..3879(total 776)\n",
      "[chunk 0] done global index750\n",
      "[chunk 4] done global index3104\n",
      "[chunk 0] finished with 776 answers\n",
      "[chunk 5] running indices 3880..4655(total 776)\n",
      "[chunk 5] done global index3880\n",
      "[chunk 5] done global index3930\n",
      "[chunk 5] done global index3980\n",
      "[chunk 3] done global index2528\n",
      "[chunk 5] done global index4030\n",
      "[chunk 2] done global index2102\n",
      "[chunk 4] done global index3154\n",
      "[chunk 5] done global index4080\n",
      "[chunk 3] done global index2578\n",
      "[chunk 3] done global index2628\n",
      "[chunk 4] done global index3204\n",
      "[chunk 2] done global index2152\n",
      "[chunk 5] done global index4130\n",
      "[chunk 3] done global index2678\n",
      "[chunk 4] done global index3254\n",
      "[chunk 3] done global index2728\n",
      "[chunk 2] done global index2202\n",
      "[chunk 5] done global index4180\n",
      "[chunk 3] done global index2778\n",
      "[chunk 4] done global index3304\n",
      "[chunk 3] done global index2828\n",
      "[chunk 4] done global index3354\n",
      "[chunk 5] done global index4230\n",
      "[chunk 3] done global index2878\n",
      "[chunk 2] done global index2252\n",
      "[chunk 4] done global index3404\n",
      "[chunk 3] done global index2928\n",
      "[chunk 5] done global index4280\n",
      "[chunk 3] done global index2978\n",
      "[chunk 4] done global index3454\n",
      "[chunk 5] done global index4330\n",
      "[chunk 2] done global index2302\n",
      "[chunk 5] done global index4380\n",
      "[chunk 3] done global index3028\n",
      "[chunk 5] done global index4430\n",
      "[chunk 5] done global index4480\n",
      "[chunk 4] done global index3504\n",
      "[chunk 2] finished with 776 answers\n",
      "[chunk 6] running indices 4656..5431(total 776)\n",
      "[chunk 6] done global index4656\n",
      "[chunk 4] done global index3554\n",
      "[chunk 5] done global index4530\n",
      "[chunk 3] done global index3078\n",
      "[chunk 6] done global index4706\n",
      "[chunk 4] done global index3604\n",
      "[chunk 5] done global index4580\n",
      "[chunk 4] done global index3654\n",
      "[chunk 5] done global index4630\n",
      "[chunk 3] finished with 776 answers\n",
      "[chunk 7] running indices 5432..6207(total 776)\n",
      "[chunk 7] done global index5432\n",
      "[chunk 4] done global index3704\n",
      "[chunk 5] finished with 776 answers\n",
      "[chunk 4] done global index3754\n",
      "[chunk 4] done global index3804\n",
      "[chunk 7] done global index5482\n",
      "[chunk 4] done global index3854\n",
      "[chunk 4] finished with 776 answers\n",
      "[chunk 6] done global index4756\n",
      "[chunk 7] done global index5532\n",
      "[chunk 7] done global index5582\n",
      "[chunk 6] done global index4806\n",
      "[chunk 7] done global index5632\n",
      "[chunk 7] done global index5682\n",
      "[chunk 6] done global index4856\n",
      "[chunk 7] done global index5732\n",
      "[chunk 7] done global index5782\n",
      "[chunk 6] done global index4906\n",
      "[chunk 7] done global index5832\n",
      "[chunk 6] done global index4956\n",
      "[chunk 7] done global index5882\n",
      "[chunk 6] done global index5006\n",
      "[chunk 7] done global index5932\n",
      "[chunk 7] done global index5982\n",
      "[chunk 6] done global index5056\n",
      "[chunk 7] done global index6032\n",
      "[chunk 6] done global index5106\n",
      "[chunk 7] done global index6082\n",
      "[chunk 7] done global index6132\n",
      "[chunk 6] done global index5156\n",
      "[chunk 7] done global index6182\n",
      "[chunk 7] finished with 776 answers\n",
      "[chunk 6] done global index5206\n",
      "[chunk 6] done global index5256\n",
      "[chunk 6] done global index5306\n",
      "[chunk 6] done global index5356\n",
      "[chunk 6] done global index5406\n",
      "[chunk 6] finished with 776 answers\n",
      "Wrote 6208 answers to cse_476_final_project_answers.json and validated format successfully.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Generate a placeholder answer file that matches the expected auto-grader format.\n",
    "\n",
    "Replace the placeholder logic inside `build_answers()` with your own agent loop\n",
    "before submitting so the ``output`` fields contain your real predictions.\n",
    "\n",
    "Reads the input questions from cse_476_final_project_test_data.json and writes\n",
    "an answers JSON file where each entry contains a string under the \"output\" key.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "INPUT_PATH = Path(\"cse_476_final_project_test_data.json\")\n",
    "OUTPUT_PATH = Path(\"cse_476_final_project_answers.json\")\n",
    "\n",
    "\n",
    "def load_questions(path: Path) -> List[Dict[str, Any]]:\n",
    "    with path.open(\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Input file must contain a list of question objects.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_answers(questions: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "    \"\"\" use agent to generate answer\n",
    "    split into 8 chubks which run parallel\"\"\"\n",
    "\n",
    "    n = len(questions)\n",
    "    num_chunks = 8\n",
    "    chunk_size = math.ceil(n/num_chunks)\n",
    "    \n",
    "    jobs: List[Tuple[int,int,int]]=[]\n",
    "    for chunk_id in range(num_chunks):\n",
    "        start_idx = chunk_id * chunk_size\n",
    "        end_idx = min((chunk_id+1)* chunk_size , n)\n",
    "        if start_idx >= n:\n",
    "            break\n",
    "        jobs.append((start_idx, end_idx, chunk_id))\n",
    "        \n",
    "    print(f\"Total questoins: {n}\")\n",
    "    print(\"Jobs (start_idx, end_idx, chunk_id):\")\n",
    "    for j in jobs:\n",
    "        print(\" \", j)\n",
    "\n",
    "\n",
    "    def worker(args: Tuple[int,int,int])-> List[Tuple[int,str]]:\n",
    "        \"\"\" worker fro single chunk: run agent on questions\"\"\"\n",
    "        start_idx, end_idx, chunk_id = args\n",
    "        print(f\"[chunk {chunk_id}] running indices {start_idx}..{end_idx-1}(total {end_idx-start_idx})\")\n",
    "        local_results: List[Tuple[int, str]]=[]\n",
    "        for i in range(start_idx, end_idx):\n",
    "            ex = questions[i]\n",
    "\n",
    "            # use agent to solve questions\n",
    "            raw_ans = agent_solve(ex, sc_samples=2, use_reflection=False)\n",
    "\n",
    "            # clean\n",
    "            out_str = extract_final_answer(raw_ans)\n",
    "            local_results.append((i, out_str))\n",
    "\n",
    "            #prog log\n",
    "            if( i - start_idx)% 50 ==0:\n",
    "                print(f\"[chunk {chunk_id}] done global index{i}\")\n",
    "        \n",
    "        print(f\"[chunk {chunk_id}] finished with {len(local_results)} answers\")\n",
    "        return local_results\n",
    "\n",
    "\n",
    "        # run all chunk simultaneously\n",
    "    max_workers = min(4, len(jobs))\n",
    "    print(f\"running max workers = {max_workers}\")\n",
    "\n",
    "    all_pairs: List[Tuple[int, str]] =[]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as pool:\n",
    "        futures = {pool.submit(worker, job): job for job in jobs}\n",
    "        for fut in as_completed(futures):\n",
    "            chunk_result = fut.result()\n",
    "            all_pairs.extend(chunk_result)\n",
    "\n",
    "    all_pairs.sort(key = lambda x: x[0])\n",
    "    if len(all_pairs) != n:\n",
    "        raise ValueError(f\"Expected {n} answers, got {len(all_pairs)} from chunks\")\n",
    "\n",
    "    answers: List[Dict[str,str]] = [{\"output\": \"\"} for _ in range(n)]\n",
    "    for idx, out_str in all_pairs:\n",
    "        answers[idx][\"output\"] = out_str\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def validate_results(\n",
    "    questions: List[Dict[str, Any]], answers: List[Dict[str, Any]]\n",
    ") -> None:\n",
    "    if len(questions) != len(answers):\n",
    "        raise ValueError(\n",
    "            f\"Mismatched lengths: {len(questions)} questions vs {len(answers)} answers.\"\n",
    "        )\n",
    "    for idx, answer in enumerate(answers):\n",
    "        if \"output\" not in answer:\n",
    "            raise ValueError(f\"Missing 'output' field for answer index {idx}.\")\n",
    "        if not isinstance(answer[\"output\"], str):\n",
    "            raise TypeError(\n",
    "                f\"Answer at index {idx} has non-string output: {type(answer['output'])}\"\n",
    "            )\n",
    "        if len(answer[\"output\"]) >= 5000:\n",
    "            raise ValueError(\n",
    "                f\"Answer at index {idx} exceeds 5000 characters \"\n",
    "                f\"({len(answer['output'])} chars). Please make sure your answer does not include any intermediate results.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    questions = load_questions(INPUT_PATH)\n",
    "    answers = build_answers(questions)\n",
    "\n",
    "    with OUTPUT_PATH.open(\"w\") as fp:\n",
    "        json.dump(answers, fp, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with OUTPUT_PATH.open(\"r\") as fp:\n",
    "        saved_answers = json.load(fp)\n",
    "    validate_results(questions, saved_answers)\n",
    "    print(\n",
    "        f\"Wrote {len(answers)} answers to {OUTPUT_PATH} \"\n",
    "        \"and validated format successfully.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f5725f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 6208\n",
      "First row: {'output': '2'}\n",
      "Last row: {'output': '- **Stack:** Yellow block → Orange block → Blue block'}\n"
     ]
    }
   ],
   "source": [
    "# check if files have merged correct\n",
    "with open(\"final_project_result2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Rows:\", len(data))\n",
    "print(\"First row:\", data[0])\n",
    "print(\"Last row:\", data[-1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379ba9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc62e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
